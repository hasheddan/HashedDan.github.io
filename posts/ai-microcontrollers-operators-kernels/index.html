<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
  How AI on Microcontrollers Actually Works: Operators and Kernels · Daniel Mangum
</title>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Daniel Mangum">
<meta name="description" content="The buzz around “edge AI”, which means something slightly different to almost everyone you talk to, is well past reaching a fever pitch. Regardless of what edge AI means to you, the one commonality is typically that the hardware on which inference is being performed is constrained in one or more dimensions, whether it be compute, memory, or network bandwidth. Perhaps the most constrained of these platforms are microcontrollers.
I have found that, while there is much discourse around &ldquo;running AI&rdquo; (i.">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://danielmangum.com/images/twitter-card.png"/>

<meta name="twitter:title" content="How AI on Microcontrollers Actually Works: Operators and Kernels"/>
<meta name="twitter:description" content="The buzz around “edge AI”, which means something slightly different to almost everyone you talk to, is well past reaching a fever pitch. Regardless of what edge AI means to you, the one commonality is typically that the hardware on which inference is being performed is constrained in one or more dimensions, whether it be compute, memory, or network bandwidth. Perhaps the most constrained of these platforms are microcontrollers.
I have found that, while there is much discourse around &ldquo;running AI&rdquo; (i."/>

<meta property="og:title" content="How AI on Microcontrollers Actually Works: Operators and Kernels" />
<meta property="og:description" content="The buzz around “edge AI”, which means something slightly different to almost everyone you talk to, is well past reaching a fever pitch. Regardless of what edge AI means to you, the one commonality is typically that the hardware on which inference is being performed is constrained in one or more dimensions, whether it be compute, memory, or network bandwidth. Perhaps the most constrained of these platforms are microcontrollers.
I have found that, while there is much discourse around &ldquo;running AI&rdquo; (i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://danielmangum.com/posts/ai-microcontrollers-operators-kernels/" /><meta property="og:image" content="https://danielmangum.com/images/twitter-card.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-06-30T01:41:34-06:00" />
<meta property="article:modified_time" content="2025-06-30T01:41:34-06:00" />




<link rel="canonical" href="https://danielmangum.com/posts/ai-microcontrollers-operators-kernels/">


<link rel="preload" href="https://danielmangum.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="https://danielmangum.com/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css" integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="https://danielmangum.com/css/coder-dark.min.216e36d3eaf6f4cdfd67dc1200c49a8169e6478102977b3e9ac51a064c57054c.css" integrity="sha256-IW420&#43;r29M39Z9wSAMSagWnmR4ECl3s&#43;msUaBkxXBUw=" crossorigin="anonymous" media="screen" />
  



 
  
    
    <link rel="stylesheet" href="https://danielmangum.com/css/custom.min.96ad7294e087b3b0719f71d369346642c5ad661660899f0b35025c5b10a70230.css" integrity="sha256-lq1ylOCHs7Bxn3HTaTRmQsWtZhZgiZ8LNQJcWxCnAjA=" crossorigin="anonymous" media="screen" />
  





<link rel="icon" type="image/png" href="https://danielmangum.com/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="https://danielmangum.com/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="https://danielmangum.com/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://danielmangum.com/images/apple-touch-icon.png">

<link rel="manifest" href="https://danielmangum.com/site.webmanifest">
<link rel="mask-icon" href="https://danielmangum.com/images/safari-pinned-tab.svg" color="#5bbad5">




<meta name="generator" content="Hugo 0.111.3">





  </head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://danielmangum.com/">
      Daniel Mangum
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://danielmangum.com/categories/risc-v-bytes/">[RISC-V Bytes]</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://danielmangum.com/risc-v-tips/">[RISC-V Tips]</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://danielmangum.com/posts/">[Blog]</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://danielmangum.com/about/">[About]</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://danielmangum.com/posts/ai-microcontrollers-operators-kernels/">
              How AI on Microcontrollers Actually Works: Operators and Kernels
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2025-06-30T01:41:34-06:00">
                June 30, 2025
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              13-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <p>The buzz around “edge AI”, which means something slightly different to almost
everyone you talk to, is well past reaching a fever pitch. Regardless of what
edge AI means to you, the one commonality is typically that the hardware on
which inference is being performed is constrained in one or more dimensions,
whether it be compute, memory, or network bandwidth. Perhaps the most
constrained of these platforms are microcontrollers.</p>
<p>I have found that, while there is much discourse around &ldquo;running AI&rdquo; (i.e.
performing inference) on microcontrollers, there is a general lack of
information about what these systems are actually capable of, and how new
hardware advancements impact that equation. It is my hope with this series to
peel back some of the layers of terminology and explore what actually happens
between supplying inputs to a model and receiving outputs. Along the way, we&rsquo;ll
ground our exploration in performing inference with real models on real
constrained hardware.</p>
<p>While &ldquo;weights&rdquo; get the majority of the attention with AI models, they alone are
not sufficient for performing inference. Depending on how a model is distributed
and what runtime is used, additional data or metadata may be supplied alongside
the model, or may be defined explicitly in software that interacts with the
weights. The most popular runtime for microcontrollers is <a href="https://ai.google.dev/edge/litert/microcontrollers/overview">Tensorflow Lite for
Microcontrollers
(<code>tflite-micro</code>)</a>,
which is an optimized version of <a href="https://ai.google.dev/edge/litert">Tensorflow
Lite</a>.</p>
<blockquote>
<p>Note: Google recently <a href="https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/">rebranded Tensorflow Lite to
LiteRT</a>,
and <code>tflite-micro</code> to LiteRT for Microcontrollers.</p>
</blockquote>
<p><code>tflite-micro</code> uses the <code>.tflite</code> file format, which encodes data using
<a href="https://flatbuffers.dev/">FlatBuffers</a>. Unlike some other model file formats,
<code>.tflite</code> files include not only the tensors that encapuslate model weights, but
also the <strong>computation graph</strong>, which informs the runtime of what operations to
use when performing inference. In order to do so, there needs to be a defined
set of <strong>operators</strong>. This is somewhat analagous to instructions defined in an
<a href="https://en.wikipedia.org/wiki/Instruction_set_architecture">instruction set architecture
(ISA)</a> for a
processor. With an ISA, a compiler will take a higher level programming language
and map the behavior onto instructions available in the ISA. Tensorflow supports
an extensive set of <a href="https://www.tensorflow.org/mlir/tf_ops">built-in
operators</a>, while Tensorflow Lite, and
thus <code>tflite-micro</code>, supports <a href="https://www.tensorflow.org/mlir/tfl_ops">only a
subset</a>.</p>
<p>Continuing the analaogy, many processors implement specific versions of the <a href="https://en.wikipedia.org/wiki/ARM_architecture_family">ARM
architecture</a>, but that
doesn&rsquo;t mean that processors implementing the same ISA are equivalent. Every
instruction that is supported has to be implemented in hardware, and decisions
about how the processor is designed can impact performance on multiple
dimensions. Similarly, while Tensorflow Lite defines a set of operators, the
implementation of those operators, which are referred to as <strong>kernels</strong>, may vary.
Kernels are implemented in software, but depending on the underlying hardware, a
kernel might take many instructions to execute, or may be able to optimized to
leverage dedicated hardware support.</p>
<p>A simple example is the <a href="https://www.tensorflow.org/mlir/tfl_ops#tfladd_tfladdop">addition operator
(<code>TFL::AddOp</code>)</a>. We&rsquo;ll
cover how operators and kernels are registered and invoked in a future post, but
let&rsquo;s start by taking a look at the default <code>tflite-micro</code> addition operator
logic.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/micro/kernels/add.cc#L168"><code>tensorflow/lite/micro/kernels/add.cc</code></a></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>TfLiteStatus AddEval(TfLiteContext* context, TfLiteNode* node) {
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">auto</span>* params = <span style="color:#fff;font-weight:bold">reinterpret_cast</span>&lt;TfLiteAddParams*&gt;(node-&gt;builtin_data);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  TFLITE_DCHECK(node-&gt;user_data != <span style="color:#fff;font-weight:bold">nullptr</span>);
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">const</span> OpDataAdd* data = <span style="color:#fff;font-weight:bold">static_cast</span>&lt;<span style="color:#fff;font-weight:bold">const</span> OpDataAdd*&gt;(node-&gt;user_data);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">const</span> TfLiteEvalTensor* input1 =
</span></span><span style="display:flex;"><span>      tflite::micro::GetEvalInput(context, node, kAddInputTensor1);
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">const</span> TfLiteEvalTensor* input2 =
</span></span><span style="display:flex;"><span>      tflite::micro::GetEvalInput(context, node, kAddInputTensor2);
</span></span><span style="display:flex;"><span>  TfLiteEvalTensor* output =
</span></span><span style="display:flex;"><span>      tflite::micro::GetEvalOutput(context, node, kAddOutputTensor);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">if</span> (output-&gt;type == kTfLiteFloat32 || output-&gt;type == kTfLiteInt32) {
</span></span><span style="display:flex;"><span>    TF_LITE_ENSURE_OK(
</span></span><span style="display:flex;"><span>        context, EvalAdd(context, node, params, data, input1, input2, output));
</span></span><span style="display:flex;"><span>  } <span style="color:#fff;font-weight:bold">else</span> <span style="color:#fff;font-weight:bold">if</span> (output-&gt;type == kTfLiteInt8 || output-&gt;type == kTfLiteInt16) {
</span></span><span style="display:flex;"><span>    TF_LITE_ENSURE_OK(context, EvalAddQuantized(context, node, params, data,
</span></span><span style="display:flex;"><span>                                                input1, input2, output));
</span></span><span style="display:flex;"><span>  } <span style="color:#fff;font-weight:bold">else</span> {
</span></span><span style="display:flex;"><span>    MicroPrintf(<span style="color:#0ff;font-weight:bold">&#34;Type %s (%d) not supported.&#34;</span>, TfLiteTypeGetName(output-&gt;type),
</span></span><span style="display:flex;"><span>                output-&gt;type);
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> kTfLiteError;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">return</span> kTfLiteOk;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>TFLMRegistration Register_ADD() {
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">return</span> tflite::micro::RegisterOp(AddInit, AddPrepare, AddEval);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>As can be observed in <code>AddEval()</code>, the type of output we are expecting
influences the implementation of the operator. To illustrate how the underlying
hardware impacts performance, let&rsquo;s focus on the case in which we expect
<code>kTfLiteInt8</code> (signed 8-bit integer) or <code>kTfLiteInt16</code> (signed 16-bit integer)
output, meaning that we&rsquo;ll call <code>EvalAddQuantized()</code>.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/micro/kernels/add.cc#L91"><code>tensorflow/lite/micro/kernels/add.cc</code></a></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>TfLiteStatus EvalAddQuantized(TfLiteContext* context, TfLiteNode* node,
</span></span><span style="display:flex;"><span>                              TfLiteAddParams* params, <span style="color:#fff;font-weight:bold">const</span> OpDataAdd* data,
</span></span><span style="display:flex;"><span>                              <span style="color:#fff;font-weight:bold">const</span> TfLiteEvalTensor* input1,
</span></span><span style="display:flex;"><span>                              <span style="color:#fff;font-weight:bold">const</span> TfLiteEvalTensor* input2,
</span></span><span style="display:flex;"><span>                              TfLiteEvalTensor* output) {
</span></span><span style="display:flex;"><span>  tflite::ArithmeticParams op_params = {};
</span></span><span style="display:flex;"><span>  op_params.left_shift = data-&gt;left_shift;
</span></span><span style="display:flex;"><span>  op_params.input1_offset = data-&gt;input1_offset;
</span></span><span style="display:flex;"><span>  op_params.input1_multiplier = data-&gt;input1_multiplier;
</span></span><span style="display:flex;"><span>  op_params.input1_shift = data-&gt;input1_shift;
</span></span><span style="display:flex;"><span>  op_params.input2_offset = data-&gt;input2_offset;
</span></span><span style="display:flex;"><span>  op_params.input2_multiplier = data-&gt;input2_multiplier;
</span></span><span style="display:flex;"><span>  op_params.input2_shift = data-&gt;input2_shift;
</span></span><span style="display:flex;"><span>  op_params.output_offset = data-&gt;output_offset;
</span></span><span style="display:flex;"><span>  op_params.output_multiplier = data-&gt;output_multiplier;
</span></span><span style="display:flex;"><span>  op_params.output_shift = data-&gt;output_shift;
</span></span><span style="display:flex;"><span>  SetActivationParams(data-&gt;output_activation_min, data-&gt;output_activation_max,
</span></span><span style="display:flex;"><span>                      &amp;op_params);
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">bool</span> need_broadcast = reference_ops::ProcessBroadcastShapes(
</span></span><span style="display:flex;"><span>      tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>      tflite::micro::GetTensorShape(input2), &amp;op_params);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">switch</span> (output-&gt;type) {
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">case</span> kTfLiteInt8: {
</span></span><span style="display:flex;"><span>      <span style="color:#fff;font-weight:bold">if</span> (need_broadcast) {
</span></span><span style="display:flex;"><span>        reference_integer_ops::BroadcastAdd4DSlow(
</span></span><span style="display:flex;"><span>            op_params, tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input1),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorShape(input2),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input2),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorShape(output),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(output));
</span></span><span style="display:flex;"><span>      } <span style="color:#fff;font-weight:bold">else</span> {
</span></span><span style="display:flex;"><span>        reference_integer_ops::Add(
</span></span><span style="display:flex;"><span>            op_params, tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input1),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorShape(input2),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input2),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorShape(output),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(output));
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>      <span style="color:#fff;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">case</span> kTfLiteInt16: {
</span></span><span style="display:flex;"><span>      <span style="color:#fff;font-weight:bold">if</span> (need_broadcast) {
</span></span><span style="display:flex;"><span>        reference_ops::BroadcastAdd4DSlow(
</span></span><span style="display:flex;"><span>            op_params, tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int16_t</span>&gt;(input1),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorShape(input2),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int16_t</span>&gt;(input2),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorShape(output),
</span></span><span style="display:flex;"><span>            tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int16_t</span>&gt;(output));
</span></span><span style="display:flex;"><span>      } <span style="color:#fff;font-weight:bold">else</span> {
</span></span><span style="display:flex;"><span>        reference_ops::Add(op_params, tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>                           tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int16_t</span>&gt;(input1),
</span></span><span style="display:flex;"><span>                           tflite::micro::GetTensorShape(input2),
</span></span><span style="display:flex;"><span>                           tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int16_t</span>&gt;(input2),
</span></span><span style="display:flex;"><span>                           tflite::micro::GetTensorShape(output),
</span></span><span style="display:flex;"><span>                           tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int16_t</span>&gt;(output),
</span></span><span style="display:flex;"><span>                           <span style="color:#fff;font-weight:bold">false</span>);
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>      <span style="color:#fff;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">default</span>:
</span></span><span style="display:flex;"><span>      MicroPrintf(<span style="color:#0ff;font-weight:bold">&#34;Type %s (%d) not supported.&#34;</span>,
</span></span><span style="display:flex;"><span>                  TfLiteTypeGetName(output-&gt;type), output-&gt;type);
</span></span><span style="display:flex;"><span>      <span style="color:#fff;font-weight:bold">return</span> kTfLiteError;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">return</span> kTfLiteOk;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>For <code>kTfLiteInt8</code> output when broadcast is not required, we make a call to
<code>reference_integer_ops::Add()</code>.</p>
<blockquote>
<p>&ldquo;Broadcasting&rdquo; is the process of making arrays to have compatible shapes for
arithmetic operations. For example, <a href="https://en.wikipedia.org/wiki/Matrix_addition">matrix
addition</a> requires that the two
input matrices have the same dimensions.</p>
</blockquote>
<p><a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/kernels/internal/reference/add.h#L31"><code>tensorflow/lite/kernels/internal/reference/add.h</code></a></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">template</span> &lt;<span style="color:#fff;font-weight:bold">typename</span> T&gt;
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">inline</span> <span style="color:#fff;font-weight:bold">void</span> Add(<span style="color:#fff;font-weight:bold">const</span> ArithmeticParams&amp; params,
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">const</span> RuntimeShape&amp; input1_shape, <span style="color:#fff;font-weight:bold">const</span> T* input1_data,
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">const</span> RuntimeShape&amp; input2_shape, <span style="color:#fff;font-weight:bold">const</span> T* input2_data,
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">const</span> RuntimeShape&amp; output_shape, T* output_data) {
</span></span><span style="display:flex;"><span>  T activation_min, activation_max;
</span></span><span style="display:flex;"><span>  GetActivationParams(params, &amp;activation_min, &amp;activation_max);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int</span> flat_size =
</span></span><span style="display:flex;"><span>      MatchingElementsSize(input1_shape, input2_shape, output_shape);
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; flat_size; ++i) {
</span></span><span style="display:flex;"><span>    output_data[i] = ActivationFunctionWithMinMax(
</span></span><span style="display:flex;"><span>        input1_data[i] + input2_data[i], activation_min, activation_max);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>As you might expect, this implementation effectively boils down to iterating
through the two input tensors and calling <code>input1_data[i] + input2_data[i]</code>.
This can be thought of as a lowest common denominator implementation in that it
doesn&rsquo;t leverage any hardware-specific functionality; any processor can perform
sequential addition. However, as evidenced by the effectively unlimited demand
in the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">Graphics Processing Unit
(GPU)</a> market, there are
significant performance gains to be had by parallelizing operations in hardware.
Fortunately, many of the operations that are necessary for performing inference
are <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">&ldquo;embarassingly
parallel&rdquo;</a>. For example,
rather than iterating through tensors to perform sequential addition, which may
take many processor cycles, we could point a processor to the two inputs and, if
supported by the hardware, the entire matrix addition operation could be
completed in a &ldquo;single&rdquo; cycle.</p>
<blockquote>
<p>It is unlikely that the operation would literally take one cycle given the
complexity of modern processors, but the point is that the runtime of the
entire operation could be reduced to the same order of magnitude of cycles as
one step of the sequential addition implementation.</p>
</blockquote>
<p>Obviously microcontrollers don&rsquo;t have massive GPUs like those installed in cloud
provider datacenters. However, many do implement architecture extensions that
enable these common operations to be accelerated. Because Tensorflow Lite
enables different kernel implementations, this hardware acceleration can be
leveraged when supported.</p>
<p>Many microcontrollers implement <a href="https://en.wikipedia.org/wiki/ARM_Cortex-M">Arm
Cortex-M</a> cores. For example, chips
like the Raspberry Pi <a href="https://www.raspberrypi.com/products/rp2350/">RP2350</a> and
Nordic Semiconductor <a href="https://www.nordicsemi.com/Products/nRF54H20">nRF54H20</a>
implement multiple Arm
<a href="https://en.wikipedia.org/wiki/ARM_Cortex-M#Cortex-M33">Cortex-M33</a> cores. The
former implements the Armv8-M Digital Signal Processing (DSP) Extension, which
adds support for <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">Single Instruction Multiple Data
(SIMD)</a>
instructions. More capable chips, like the <a href="https://alifsemi.com/ensemble-e3-series/">Alif Ensemble
E3</a> implement
<a href="https://en.wikipedia.org/wiki/ARM_Cortex-M#Cortex-M55">Cortex-M55</a> cores, with
support for the Armv8-M Vector Extension (MVE), also referred to as <a href="https://www.arm.com/technologies/helium">Arm
Helium</a>. The E3 also includes dedicated
accelerators in the form of <a href="https://developer.arm.com/documentation/109267/0102/Arm-Ethos-U-NPU">Arm&rsquo;s Ethos-U Neural Processing Units
(NPU)</a>.</p>
<p>Arm provides software that allows for hardware that supports one or more of
these extensions to accelerate Tensorflow Lite kernel implementations. For
example, the <a href="https://github.com/ARM-software/CMSIS-NN">CMSIS-NN</a> library offers
kernel implementations that do not leverage optimization (i.e. <a href="https://github.com/ARM-software/CMSIS-NN?tab=readme-ov-file#pure-c">pure
C</a>),
leverage <a href="https://github.com/ARM-software/CMSIS-NN?tab=readme-ov-file#dsp-extension">just the DSP
extension</a>,
or <a href="https://github.com/ARM-software/CMSIS-NN?tab=readme-ov-file#mve-extension">leverage the MVE
extension</a>
(which requires the implementation of the DSP extension). Tensorflow Lite has
kernel &ldquo;ports&rdquo; that integrate the CMSIS-NN functionality. Let&rsquo;s take a look at
how the add operation differs when using CMSIS-NN kernels.</p>
<p>The
<a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/micro/kernels/cmsis_nn/add.cc#L408">setup</a>
looks largely the same as the first add operation kernel we observed. However,
when we reach <code>EvalAddQuantizedInt8()</code>, we can start to see where hardware
acceleration is leveraged.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/micro/kernels/cmsis_nn/add.cc#L124"><code>tensorflow/lite/micro/kernels/cmsis_nn/add.cc</code></a></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>TfLiteStatus EvalAddQuantizedInt8(TfLiteContext* context, TfLiteNode* node,
</span></span><span style="display:flex;"><span>                                  TfLiteAddParams* params, <span style="color:#fff;font-weight:bold">const</span> OpData* data,
</span></span><span style="display:flex;"><span>                                  <span style="color:#fff;font-weight:bold">const</span> TfLiteEvalTensor* input1,
</span></span><span style="display:flex;"><span>                                  <span style="color:#fff;font-weight:bold">const</span> TfLiteEvalTensor* input2,
</span></span><span style="display:flex;"><span>                                  TfLiteEvalTensor* output) {
</span></span><span style="display:flex;"><span>  tflite::ArithmeticParams op_params;
</span></span><span style="display:flex;"><span>  UpdateOpParams(&amp;op_params, data);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">bool</span> need_broadcast = reference_ops::ProcessBroadcastShapes(
</span></span><span style="display:flex;"><span>      tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>      tflite::micro::GetTensorShape(input2), &amp;op_params);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">if</span> (need_broadcast) {
</span></span><span style="display:flex;"><span>    reference_integer_ops::BroadcastAdd4DSlow(
</span></span><span style="display:flex;"><span>        op_params, tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input1),
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorShape(input2),
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input2),
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorShape(output),
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(output));
</span></span><span style="display:flex;"><span>  } <span style="color:#fff;font-weight:bold">else</span> {
</span></span><span style="display:flex;"><span>    arm_elementwise_add_s8(
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input1),
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(input2), op_params.input1_offset,
</span></span><span style="display:flex;"><span>        op_params.input1_multiplier, op_params.input1_shift,
</span></span><span style="display:flex;"><span>        op_params.input2_offset, op_params.input2_multiplier,
</span></span><span style="display:flex;"><span>        op_params.input2_shift, op_params.left_shift,
</span></span><span style="display:flex;"><span>        tflite::micro::GetTensorData&lt;<span style="color:#fff;font-weight:bold">int8_t</span>&gt;(output), op_params.output_offset,
</span></span><span style="display:flex;"><span>        op_params.output_multiplier, op_params.output_shift,
</span></span><span style="display:flex;"><span>        op_params.quantized_activation_min, op_params.quantized_activation_max,
</span></span><span style="display:flex;"><span>        MatchingElementsSize(tflite::micro::GetTensorShape(input1),
</span></span><span style="display:flex;"><span>                             tflite::micro::GetTensorShape(input2),
</span></span><span style="display:flex;"><span>                             tflite::micro::GetTensorShape(output)));
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">return</span> kTfLiteOk;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The <code>arm_elementwise_add_s8()</code> function is provided by CMSIS-NN, and the
implementation leverages different hardware functionality depending on what
extensions are available.</p>
<p><a href="https://github.com/ARM-software/CMSIS-NN/blob/dbcb869c2f147f89f661f977cb8687b5a656a841/Source/BasicMathFunctions/arm_elementwise_add_s8.c#L52"><code>Source/BasicMathFunctions/arm_elementwise_add_s8.c</code></a></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>arm_cmsis_nn_status arm_elementwise_add_s8(<span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int8_t</span> *input_1_vect,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int8_t</span> *input_2_vect,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> input_1_offset,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> input_1_mult,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> input_1_shift,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> input_2_offset,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> input_2_mult,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> input_2_shift,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> left_shift,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">int8_t</span> *output,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> out_offset,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> out_mult,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> out_shift,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> out_activation_min,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> out_activation_max,
</span></span><span style="display:flex;"><span>                                           <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">int32_t</span> block_size)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold">#if defined(ARM_MATH_MVEI)
</span></span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold"></span>    <span style="color:#fff;font-weight:bold">int32_t</span> count = block_size;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">while</span> (count &gt; <span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        int32x4_t vect_1;
</span></span><span style="display:flex;"><span>        int32x4_t vect_2;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        mve_pred16_t p = vctp32q((<span style="color:#fff;font-weight:bold">uint32_t</span>)count);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = vldrbq_z_s32(input_1_vect, p);
</span></span><span style="display:flex;"><span>        vect_2 = vldrbq_z_s32(input_2_vect, p);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = vaddq_s32(vect_1, vdupq_n_s32(input_1_offset));
</span></span><span style="display:flex;"><span>        vect_2 = vaddq_s32(vect_2, vdupq_n_s32(input_2_offset));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = vshlq_r_s32(vect_1, left_shift);
</span></span><span style="display:flex;"><span>        vect_2 = vshlq_r_s32(vect_2, left_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = arm_requantize_mve(vect_1, input_1_mult, input_1_shift);
</span></span><span style="display:flex;"><span>        vect_2 = arm_requantize_mve(vect_2, input_2_mult, input_2_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = vaddq_s32(vect_1, vect_2);
</span></span><span style="display:flex;"><span>        vect_1 = arm_requantize_mve(vect_1, out_mult, out_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = vaddq_n_s32(vect_1, out_offset);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        vect_1 = vmaxq_s32(vect_1, vdupq_n_s32(out_activation_min));
</span></span><span style="display:flex;"><span>        vect_1 = vminq_s32(vect_1, vdupq_n_s32(out_activation_max));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_1_vect += <span style="color:#ff0;font-weight:bold">4</span>;
</span></span><span style="display:flex;"><span>        input_2_vect += <span style="color:#ff0;font-weight:bold">4</span>;
</span></span><span style="display:flex;"><span>        vstrbq_p_s32(output, vect_1, p);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        output += <span style="color:#ff0;font-weight:bold">4</span>;
</span></span><span style="display:flex;"><span>        count -= <span style="color:#ff0;font-weight:bold">4</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold">#else
</span></span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold"></span>    <span style="color:#fff;font-weight:bold">int32_t</span> loop_count;
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">int32_t</span> input_1;
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">int32_t</span> input_2;
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">int32_t</span> sum;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#0f0;font-weight:bold">#if defined(ARM_MATH_DSP)
</span></span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold"></span>    <span style="color:#fff;font-weight:bold">int32_t</span> a_1, b_1, a_2, b_2;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">int32_t</span> offset_1_packed, offset_2_packed;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">int8_t</span> r1, r2, r3, r4;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    offset_1_packed = (input_1_offset &lt;&lt; <span style="color:#ff0;font-weight:bold">16U</span>) | (input_1_offset &amp; <span style="color:#ff0;font-weight:bold">0x0FFFFL</span>);
</span></span><span style="display:flex;"><span>    offset_2_packed = (input_2_offset &lt;&lt; <span style="color:#ff0;font-weight:bold">16U</span>) | (input_2_offset &amp; <span style="color:#ff0;font-weight:bold">0x0FFFFL</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loop_count = block_size &gt;&gt; <span style="color:#ff0;font-weight:bold">2</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">while</span> (loop_count &gt; <span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* 4 outputs are calculated in one loop. The order of calculation is follows the order of output sign extension
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f">           intrinsic */</span>
</span></span><span style="display:flex;"><span>        input_1_vect = read_and_pad_reordered(input_1_vect, &amp;b_1, &amp;a_1);
</span></span><span style="display:flex;"><span>        input_2_vect = read_and_pad_reordered(input_2_vect, &amp;b_2, &amp;a_2);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        a_1 = SADD16(a_1, offset_1_packed);
</span></span><span style="display:flex;"><span>        b_1 = SADD16(b_1, offset_1_packed);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        a_2 = SADD16(a_2, offset_2_packed);
</span></span><span style="display:flex;"><span>        b_2 = SADD16(b_2, offset_2_packed);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* Sum 1 */</span>
</span></span><span style="display:flex;"><span>        input_1 = (b_1 &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_1 = arm_nn_requantize(input_1, input_1_mult, input_1_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_2 = (b_2 &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_2 = arm_nn_requantize(input_2, input_2_mult, input_2_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sum = input_1 + input_2;
</span></span><span style="display:flex;"><span>        sum = arm_nn_requantize(sum, out_mult, out_shift);
</span></span><span style="display:flex;"><span>        sum += out_offset;
</span></span><span style="display:flex;"><span>        sum = MAX(sum, out_activation_min);
</span></span><span style="display:flex;"><span>        sum = MIN(sum, out_activation_max);
</span></span><span style="display:flex;"><span>        r1 = (<span style="color:#fff;font-weight:bold">int8_t</span>)sum;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* Sum 3 */</span>
</span></span><span style="display:flex;"><span>        input_1 = ((b_1 &gt;&gt; <span style="color:#ff0;font-weight:bold">16</span>) &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_1 = arm_nn_requantize(input_1, input_1_mult, input_1_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_2 = ((b_2 &gt;&gt; <span style="color:#ff0;font-weight:bold">16</span>) &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_2 = arm_nn_requantize(input_2, input_2_mult, input_2_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sum = input_1 + input_2;
</span></span><span style="display:flex;"><span>        sum = arm_nn_requantize(sum, out_mult, out_shift);
</span></span><span style="display:flex;"><span>        sum += out_offset;
</span></span><span style="display:flex;"><span>        sum = MAX(sum, out_activation_min);
</span></span><span style="display:flex;"><span>        sum = MIN(sum, out_activation_max);
</span></span><span style="display:flex;"><span>        r3 = (<span style="color:#fff;font-weight:bold">int8_t</span>)sum;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* Sum 2 */</span>
</span></span><span style="display:flex;"><span>        input_1 = (a_1 &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_1 = arm_nn_requantize(input_1, input_1_mult, input_1_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_2 = (a_2 &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_2 = arm_nn_requantize(input_2, input_2_mult, input_2_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sum = input_1 + input_2;
</span></span><span style="display:flex;"><span>        sum = arm_nn_requantize(sum, out_mult, out_shift);
</span></span><span style="display:flex;"><span>        sum += out_offset;
</span></span><span style="display:flex;"><span>        sum = MAX(sum, out_activation_min);
</span></span><span style="display:flex;"><span>        sum = MIN(sum, out_activation_max);
</span></span><span style="display:flex;"><span>        r2 = (<span style="color:#fff;font-weight:bold">int8_t</span>)sum;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* Sum 4 */</span>
</span></span><span style="display:flex;"><span>        input_1 = ((a_1 &gt;&gt; <span style="color:#ff0;font-weight:bold">16</span>) &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_1 = arm_nn_requantize(input_1, input_1_mult, input_1_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_2 = ((a_2 &gt;&gt; <span style="color:#ff0;font-weight:bold">16</span>) &amp; <span style="color:#ff0;font-weight:bold">0x0FFFF</span>) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_2 = arm_nn_requantize(input_2, input_2_mult, input_2_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sum = input_1 + input_2;
</span></span><span style="display:flex;"><span>        sum = arm_nn_requantize(sum, out_mult, out_shift);
</span></span><span style="display:flex;"><span>        sum += out_offset;
</span></span><span style="display:flex;"><span>        sum = MAX(sum, out_activation_min);
</span></span><span style="display:flex;"><span>        sum = MIN(sum, out_activation_max);
</span></span><span style="display:flex;"><span>        r4 = (<span style="color:#fff;font-weight:bold">int8_t</span>)sum;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        arm_nn_write_s8x4_ia(&amp;output, PACK_S8x4_32x1(r1, r2, r3, r4));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        loop_count--;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    loop_count = block_size &amp; <span style="color:#ff0;font-weight:bold">0x3</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#0f0;font-weight:bold">#else
</span></span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold"></span>    loop_count = block_size;
</span></span><span style="display:flex;"><span>    <span style="color:#0f0;font-weight:bold">#endif
</span></span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold"></span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">while</span> (loop_count &gt; <span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* C = A + B */</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_1 = (*input_1_vect++ + input_1_offset) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>        input_2 = (*input_2_vect++ + input_2_offset) &lt;&lt; left_shift;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_1 = arm_nn_requantize(input_1, input_1_mult, input_1_shift);
</span></span><span style="display:flex;"><span>        input_2 = arm_nn_requantize(input_2, input_2_mult, input_2_shift);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sum = input_1 + input_2;
</span></span><span style="display:flex;"><span>        sum = arm_nn_requantize(sum, out_mult, out_shift);
</span></span><span style="display:flex;"><span>        sum += out_offset;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sum = MAX(sum, out_activation_min);
</span></span><span style="display:flex;"><span>        sum = MIN(sum, out_activation_max);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        *output++ = (<span style="color:#fff;font-weight:bold">int8_t</span>)sum;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f">/* Decrement loop counter */</span>
</span></span><span style="display:flex;"><span>        loop_count--;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold">#endif </span><span style="color:#007f7f">/* ARM_MATH_MVEI */</span><span style="color:#0f0;font-weight:bold">
</span></span></span><span style="display:flex;"><span><span style="color:#0f0;font-weight:bold"></span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> (ARM_CMSIS_NN_SUCCESS);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>For example, if the DSP extension is present, the parallel signed 16-bit
addition (<code>SAAD16</code>) instruction provided by the extension is used to reduce the
number of loops necessary by packing 8-bit signed integers into 16-bit
arguments, then calculating 4 outputs on a single iteration. If the MVE
extension is present, vector addition instructions (<code>VADD</code>) can be used
directly, making the calculation even more efficient.</p>
<p>These optimizations are made available via configuration when compiling
<code>tflite-micro</code>. They can be applied to any models that utilize the operations,
without the need to modify the model when moving from one architecture to
another. Some optimizations do require modifying a model. For example, when
using microcontrollers, such as the previously mentioned Alif Ensemble E3, that
include Arm&rsquo;s Ethos-U NPUs you can run <code>.tflite</code> models through the <a href="https://developer.arm.com/documentation/109267/0102/Tool-support-for-the-Arm-Ethos-U-NPU/Ethos-U-Vela-compiler">Vela
compiler</a>.
Converted models replace sequences of built-in operators with a custom <a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/micro/micro_mutable_op_resolver.h#L258"><code>ETHOSU</code>
operator</a>
and a <a href="https://developer.arm.com/documentation/102420/0200/Programmers-model/Command-stream?lang=en">command
stream</a>.
The application processor notifies the NPU of the address of the command stream
and other relevant data, then triggers it to perform inference.</p>
<p>Unlike the addition operator, there is no fallback kernel implementation; models
converted via the Vela compiler cannot run on microcontrollers that do not have
Ethos-U NPUs. For those that do, we can see the previously described logic in
the <code>Eval()</code> implementation for the <code>ETHOSU</code> custom operator.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/blob/3200ccd18268346d0ea965720c5599a3d051e853/tensorflow/lite/micro/kernels/ethos_u/ethosu.cc"><code>tensorflow/lite/micro/kernels/ethos_u/ethosu.cc</code></a></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
</span></span><span style="display:flex;"><span>  TFLITE_DCHECK(node-&gt;user_data != <span style="color:#fff;font-weight:bold">nullptr</span>);
</span></span><span style="display:flex;"><span>  TFLITE_DCHECK(context != <span style="color:#fff;font-weight:bold">nullptr</span>);
</span></span><span style="display:flex;"><span>  TFLITE_DCHECK(context-&gt;GetScratchBuffer != <span style="color:#fff;font-weight:bold">nullptr</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#007f7f">// Get base addresses.
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  TfLiteEvalTensor* tensor;
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">int</span> num_tensors = <span style="color:#ff0;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">void</span>* cms_data;
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">uint8_t</span> co_type;
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">int</span> result;
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">const</span> OpData* data = <span style="color:#fff;font-weight:bold">static_cast</span>&lt;<span style="color:#fff;font-weight:bold">const</span> OpData*&gt;(node-&gt;user_data);
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">uint64_t</span>* base_addrs = <span style="color:#fff;font-weight:bold">static_cast</span>&lt;<span style="color:#fff;font-weight:bold">uint64_t</span>*&gt;(
</span></span><span style="display:flex;"><span>      context-&gt;GetScratchBuffer(context, data-&gt;base_addr_idx));
</span></span><span style="display:flex;"><span>  size_t* base_addrs_size = <span style="color:#fff;font-weight:bold">static_cast</span>&lt;size_t*&gt;(
</span></span><span style="display:flex;"><span>      context-&gt;GetScratchBuffer(context, data-&gt;base_addr_size_idx));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">const</span> <span style="color:#fff;font-weight:bold">uint8_t</span>* custom_data =
</span></span><span style="display:flex;"><span>      <span style="color:#fff;font-weight:bold">static_cast</span>&lt;<span style="color:#fff;font-weight:bold">uint8_t</span> <span style="color:#fff;font-weight:bold">const</span>*&gt;(node-&gt;custom_initial_data);
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">auto</span> root = flexbuffers::GetRoot(custom_data, node-&gt;custom_initial_data_size);
</span></span><span style="display:flex;"><span>  co_type = root.AsInt8();
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">if</span> (co_type != CO_TYPE_ETHOSU) {
</span></span><span style="display:flex;"><span>    MicroPrintf(<span style="color:#0ff;font-weight:bold">&#34;CO_TYPE != ETHOSU&#34;</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> kTfLiteError;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#007f7f">// Get command stream data address.
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  tensor = context-&gt;GetEvalTensor(context, node-&gt;inputs-&gt;data[<span style="color:#ff0;font-weight:bold">0</span>]);
</span></span><span style="display:flex;"><span>  cms_data = <span style="color:#fff;font-weight:bold">reinterpret_cast</span>&lt;<span style="color:#fff;font-weight:bold">void</span>*&gt;(tensor-&gt;data.uint8);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#007f7f">// Get addresses to weights/scratch/input data.
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#fff;font-weight:bold">for</span> (i = <span style="color:#ff0;font-weight:bold">1</span>; i &lt; node-&gt;inputs-&gt;size; ++i) {
</span></span><span style="display:flex;"><span>    tensor = context-&gt;GetEvalTensor(context, node-&gt;inputs-&gt;data[i]);
</span></span><span style="display:flex;"><span>    base_addrs[num_tensors] =
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">static_cast</span>&lt;<span style="color:#fff;font-weight:bold">uint64_t</span>&gt;(<span style="color:#fff;font-weight:bold">reinterpret_cast</span>&lt;uintptr_t&gt;(tensor-&gt;data.uint8));
</span></span><span style="display:flex;"><span>    size_t byte_size = <span style="color:#ff0;font-weight:bold">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> k = <span style="color:#ff0;font-weight:bold">0</span>; k &lt; tensor-&gt;dims-&gt;size; k++) {
</span></span><span style="display:flex;"><span>      byte_size = byte_size * tensor-&gt;dims-&gt;data[k];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    base_addrs_size[num_tensors] = byte_size;
</span></span><span style="display:flex;"><span>    num_tensors++;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#007f7f">// Get addresses to output data.
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#fff;font-weight:bold">for</span> (i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; node-&gt;outputs-&gt;size; ++i) {
</span></span><span style="display:flex;"><span>    tensor = context-&gt;GetEvalTensor(context, node-&gt;outputs-&gt;data[i]);
</span></span><span style="display:flex;"><span>    base_addrs[num_tensors] =
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">static_cast</span>&lt;<span style="color:#fff;font-weight:bold">uint64_t</span>&gt;(<span style="color:#fff;font-weight:bold">reinterpret_cast</span>&lt;uintptr_t&gt;(tensor-&gt;data.uint8));
</span></span><span style="display:flex;"><span>    size_t byte_size = <span style="color:#ff0;font-weight:bold">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> k = <span style="color:#ff0;font-weight:bold">0</span>; k &lt; tensor-&gt;dims-&gt;size; k++) {
</span></span><span style="display:flex;"><span>      byte_size = byte_size * tensor-&gt;dims-&gt;data[k];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    base_addrs_size[num_tensors] = byte_size;
</span></span><span style="display:flex;"><span>    num_tensors++;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#007f7f">// When Vela optimizes a tflite file it will assign the tensors like this:
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">//
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// +-------+------------------------+  +--------+-------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// | INPUT | Description            |  | OUTPUT | Description |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// +-------+------------------------+  +--------+-------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |     0 | Ethos-U command stream |  |   0..m | Outputs     |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |     1 | TFLM model             |  +--------+-------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |     2 | TFLM arena             |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |     3 | Ethos-U fast scratch   |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |  4..n | Inputs                 |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// +-------+------------------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">//
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// This code will assign the NPU base addresses like this:
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">//
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// +--------------+----------------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// | Base address | Description          |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// +--------------+----------------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |            0 | TFLM model           |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |            1 | TFLM arena           |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |            2 | Ethos-U fast scratch |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |         3..n | Input tensors        |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// |         n..m | Output tensors       |
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// +--------------+----------------------+
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">//
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// The number of base address will be limited to 8.
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">//
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// NOTE! The command stream produced by Vela will access the IFM and OFM
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// buffers using base address 1. This means that it is not possible to point
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  <span style="color:#007f7f">// the input and output tensors outside of the TFLM arena.
</span></span></span><span style="display:flex;"><span><span style="color:#007f7f"></span>  num_tensors = std::min(num_tensors, <span style="color:#ff0;font-weight:bold">8</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">struct</span> ethosu_driver* drv = ethosu_reserve_driver();
</span></span><span style="display:flex;"><span>  result = ethosu_invoke_v3(drv, cms_data, data-&gt;cms_data_size, base_addrs,
</span></span><span style="display:flex;"><span>                            base_addrs_size, num_tensors,
</span></span><span style="display:flex;"><span>                            GetMicroContext(context)-&gt;external_context());
</span></span><span style="display:flex;"><span>  ethosu_release_driver(drv);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fff;font-weight:bold">if</span> (-<span style="color:#ff0;font-weight:bold">1</span> == result) {
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> kTfLiteError;
</span></span><span style="display:flex;"><span>  } <span style="color:#fff;font-weight:bold">else</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> kTfLiteOk;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>We&rsquo;ve now seen the full spectrum of operator optimization, from kernels that are
implemented purely in C, to those that leverage hardware instructions provided
in architecture extensions, and finally to those that offload inference to a
wholly separate processor. In future posts, we&rsquo;ll explore how operators are
encoded in <code>.tflite</code> files, and how the runtime ultimately invokes the
underlying kernels.</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
     Daniel Mangum 
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="https://danielmangum.com/js/coder.min.27afce394fb6284f521b3fbc9f6a8326342333c3092267f3944d770489876fed.js" integrity="sha256-J6/OOU&#43;2KE9SGz&#43;8n2qDJjQjM8MJImfzlE13BImHb&#43;0="></script>
  

  

  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-116820283-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  

  

  

  

  

  

  
</body>

</html>
